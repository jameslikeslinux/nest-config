---
node_role: storage
ssd_node_role: "%{lookup('node_role')}"
ssd_storage_class: data-crypt-block
ssd_volume_size: 64Gi
hdd_node_role: workstation
hdd_storage_class: nest-crypt-block
hdd_volume_size: 256Gi
service_name: "%{nest::kubernetes::service}"
cert_issuer: eyrie-ca
cert_issuer_kind: ClusterIssuer
ingress_class: nginx
objectstore_ingress_class: nginx-workstation
objectstore_fqdn: objectstore.eyrie

.placement: &placement
  nodeAffinity: &affinity
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "node-role.kubernetes.io/%{lookup('node_role')}"
            operator: Exists
  tolerations: &tolerations
    - key: "node-role.kubernetes.io/%{lookup('node_role')}"
      operator: Exists
      effect: NoSchedule

resources:
  registry_auths: {}
  backup: {}

  # Define certs for dashboard
  certs:
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: "%{nest::kubernetes::service}-dashboard-certs"
      namespace: "%{nest::kubernetes::namespace}"
    spec:
      secretName: "%{nest::kubernetes::service}-dashboard-certs"
      issuerRef:
        name: "%{lookup('cert_issuer')}"
        kind: "%{lookup('cert_issuer_kind')}"
      dnsNames:
        - "%{nest::kubernetes::fqdn}"

  objectstore-ingress:
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: "%{lookup('service_name')}-objectstore"
      namespace: "%{nest::kubernetes::namespace}"
    spec:
      ingressClassName: "%{lookup('objectstore_ingress_class')}"
      rules:
        - host: "%{lookup('objectstore_fqdn')}"
          http: &objectstore_http
            paths:
              - path: /
                backend:
                  service:
                    name: "rook-ceph-rgw-%{lookup('service_name')}-objectstore"
                    port:
                      number: 8080
                pathType: Prefix
        - host: "*.%{lookup('objectstore_fqdn')}"
          http: *objectstore_http

values:
  clusterName: "%{lookup('service_name')}"

  # Expose resources for prometheus
  monitoring:
    enabled: true

  cephClusterSpec:
    cephVersion:
      # Workaround known issue with 18.2.4 on ARM64
      # See: https://github.com/rook/rook/issues/14502
      image: quay.io/ceph/ceph:v18.2.2

    # Run on specified node_role
    placement:
      all: *placement

    # Remove resource requirements on these small, dedicated nodes
    resources: null

    dashboard:
      prometheusEndpoint: http://ceph-monitoring-prometheus:9090
      ssl: false

    # Run mon and osd on local storage, declaratively
    # See: https://rook.io/docs/rook/latest/CRDs/Cluster/pvc-cluster/#local-storage-example
    # See: https://github.com/rook/rook/blob/master/deploy/examples/cluster-on-local-pvc.yaml
    mon:
      volumeClaimTemplate:
        spec:
          storageClassName: data-crypt
          resources:
            requests:
              storage: 10Gi
    storage:
      useAllNodes: false # use storageClassDeviceSets
      onlyApplyOSDPlacement: true # control placement per set
      storageClassDeviceSets:
        - name: set1 # operator expects this name e.g. set2, set3
          count: 3
          portable: false
          placement: &ssd_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: "node-role.kubernetes.io/%{lookup('ssd_node_role')}"
                      operator: Exists
            tolerations:
              - key: "node-role.kubernetes.io/%{lookup('ssd_node_role')}"
                operator: Exists
                effect: NoSchedule
            # Avoid scheduling multiple OSD pods to the same node
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
          preparePlacement: *ssd_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: ssd
              spec:
                resources:
                  requests:
                    storage: "%{lookup('ssd_volume_size')}"
                storageClassName: "%{lookup('ssd_storage_class')}"
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
        - name: set2
          count: 3
          portable: false
          placement: &hdd_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: "node-role.kubernetes.io/%{lookup('hdd_node_role')}"
                      operator: Exists
            tolerations:
              - key: "node-role.kubernetes.io/%{lookup('hdd_node_role')}"
                operator: Exists
                effect: NoSchedule
            # Avoid scheduling multiple OSD pods to the same node
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
          preparePlacement: *hdd_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: hdd
              spec:
                resources:
                  requests:
                    storage: "%{lookup('hdd_volume_size')}"
                storageClassName: "%{lookup('hdd_storage_class')}"
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
    # end: storage
  # end: cephClusterSpec

  ingress:
    dashboard:
      host:
        name: "%{nest::kubernetes::fqdn}"
      tls:
        - hosts:
          - "%{nest::kubernetes::fqdn}"
          secretName: "%{nest::kubernetes::service}-dashboard-certs"
      ingressClassName: "%{lookup('ingress_class')}"

  toolbox:
    enabled: true
    tolerations: *tolerations
    affinity:
      nodeAffinity: *affinity

patches:
  10-pools:
    # Block
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-blockpool"
        - op: replace
          path: '/spec/deviceClass'
          value: ssd
      target:
        kind: CephBlockPool
        name: ceph-blockpool
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-block"
        - op: replace
          path: '/parameters/pool'
          value: "%{lookup('service_name')}-blockpool"
      target:
        kind: StorageClass
        name: ceph-block

    # Filesystem
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/spec/dataPools/0/deviceClass'
          value: ssd
        - op: add
          path: '/spec/metadataServer/placement'
          value: *placement
        - op: remove
          path: '/spec/metadataServer/resources'
      target:
        kind: CephFilesystem
        name: ceph-filesystem
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem-csi"
        - op: replace
          path: '/spec/filesystemName'
          value: "%{lookup('service_name')}-filesystem"
      target:
        kind: CephFilesystemSubVolumeGroup
        name: ceph-filesystem-csi
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/parameters/fsName'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/parameters/pool'
          value: "%{lookup('service_name')}-filesystem-data0"
      target:
        kind: StorageClass
        name: ceph-filesystem

    # Object
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-objectstore"
        - op: replace
          path: '/spec/dataPool/deviceClass'
          value: hdd
        - op: replace
          path: '/spec/dataPool/failureDomain'
          value: osd
        - op: replace
          path: '/spec/gateway/port'
          value: 8080
        - op: add
          path: '/spec/gateway/placement'
          value: *placement
        - op: remove
          path: '/spec/gateway/resources'
        - op: replace
          path: '/spec/hosting'
          value:
            advertiseEndpoint:
              dnsName: "%{lookup('objectstore_fqdn')}"
              port: 80
              useTls: false
      target:
        kind: CephObjectStore
        name: ceph-objectstore
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-bucket"
        - op: replace
          path: '/parameters/objectStoreName'
          value: "%{lookup('service_name')}-objectstore"
      target:
        kind: StorageClass
        name: ceph-bucket
