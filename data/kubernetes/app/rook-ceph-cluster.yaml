---
.placement: &placement
  nodeAffinity: &affinity
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
              - storage
  tolerations: &tolerations
    - key: role
      operator: Exists
      effect: NoSchedule

resources:
  # Disable Nest backup
  backup: {}

  # Define certs for dashboard
  certs:
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: ceph-dashboard-certs
      namespace: "%{nest::kubernetes::namespace}"
    spec:
      secretName: ceph-dashboard-certs
      issuerRef:
        name: eyrie
        kind: ClusterIssuer
      dnsNames:
        - ceph.eyrie

values:
  # Expose resources for prometheus
  monitoring:
    enabled: true

  cephClusterSpec:
    cephVersion:
      # Workaround known issue with 18.2.4 on ARM64
      # See: https://github.com/rook/rook/issues/14502
      image: quay.io/ceph/ceph:v18.2.2

    # Run on storage nodes
    placement:
      all: *placement

    # Remove resource requirements on these small, dedicated nodes
    resources: null

    dashboard:
      prometheusEndpoint: http://rook-ceph-monitoring-kube-prometheus:9090
      ssl: false

    # Run mon and osd on local storage, declaratively
    # See: https://rook.io/docs/rook/latest/CRDs/Cluster/pvc-cluster/#local-storage-example
    # See: https://github.com/rook/rook/blob/master/deploy/examples/cluster-on-local-pvc.yaml
    mon:
      count: 3
      allowMultiplePerNode: false
      volumeClaimTemplate:
        spec:
          storageClassName: data-crypt
          resources:
            requests:
              storage: 10Gi
    storage:
      useAllNodes: false # use storageClassDeviceSets
      onlyApplyOSDPlacement: true # control placement per set
      storageClassDeviceSets:
        - name: set1 # operator expects this name e.g. set2, set3
          count: 3   # number of storage nodes
          portable: false
          placement: &storage_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                        - storage
            tolerations: *tolerations
            # Do not schedule multiple OSD pods to the same node. In practice,
            # the storage driver will win, but here in case the storageClass changes.
            # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#more-practical-use-cases
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
          preparePlacement: *storage_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: ssd
              spec:
                resources:
                  requests:
                    storage: 64Gi
                storageClassName: data-crypt-block
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
        - name: set2
          count: 3
          portable: false
          placement: &workstation_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                        - workstation
            tolerations: *tolerations
            # Avoid scheduling multiple OSD pods to the same node
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
          preparePlacement: *workstation_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: hdd
              spec:
                resources:
                  requests:
                    storage: 256Gi
                storageClassName: nest-crypt-block
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
    # end: storage
  # end: cephClusterSpec

  # Customize Block Pool
  # Copied from default at https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
  # and modified to use SSD deviceClass
  cephBlockPools:
    - name: ceph-blockpool
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      spec:
        failureDomain: host
        replicated:
          size: 3
        deviceClass: ssd
        # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
        # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
        # enableRBDStats: true
      storageClass:
        enabled: true
        name: ceph-block
        annotations: {}
        labels: {}
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        mountOptions: []
        # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
        allowedTopologies: []
        #        - matchLabelExpressions:
        #            - key: rook-ceph-role
        #              values:
        #                - storage-node
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
        parameters:
          # (optional) mapOptions is a comma-separated list of map options.
          # For krbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
          # For nbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
          # mapOptions: lock_on_read,queue_depth=1024

          # (optional) unmapOptions is a comma-separated list of unmap options.
          # For krbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
          # For nbd options refer
          # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
          # unmapOptions: force

          # RBD image format. Defaults to "2".
          imageFormat: "2"

          # RBD image features, equivalent to OR'd bitfield value: 63
          # Available for imageFormat: "2". Older releases of CSI RBD
          # support only the `layering` feature. The Linux kernel (KRBD) supports the
          # full feature complement as of 5.4
          imageFeatures: layering

          # These secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4

  # Customize Filesystem
  # Copied from default at https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
  # and modified to use SSD deviceClass, disable MDS resources, and add MDS placement
  cephFileSystems:
    - name: ceph-filesystem
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - failureDomain: host
            replicated:
              size: 3
            # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
            name: data0
            deviceClass: ssd
        metadataServer:
          activeCount: 1
          activeStandby: true
          # resources:
          #   limits:
          #     memory: "4Gi"
          #   requests:
          #     cpu: "1000m"
          #     memory: "4Gi"
          priorityClassName: system-cluster-critical
          placement: *placement
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-filesystem
        # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        mountOptions: []
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
        parameters:
          # The secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4

  # Customize Object Store
  # Copied from default at https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
  # and modified to use HDD deviceClass, disable RGW resources, and add RGW placement
  cephObjectStores:
    - name: ceph-objectstore
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: osd
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          deviceClass: hdd
        preservePoolsOnDelete: true
        gateway:
          port: 80
          # resources:
          #   limits:
          #     memory: "2Gi"
          #   requests:
          #     cpu: "1000m"
          #     memory: "1Gi"
          # securePort: 443
          # sslCertificateRef:
          instances: 1
          priorityClassName: system-cluster-critical
          placement:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                     - key: role
                       operator: In
                       values:
                         - workstation
            tolerations: *tolerations
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          region: us-east-1
      ingress:
        # Enable an ingress for the ceph-objectstore
        enabled: false
        # annotations: {}
        # host:
        #   name: objectstore.example.com
        #   path: /
        # tls:
        # - hosts:
        #     - objectstore.example.com
        #   secretName: ceph-objectstore-tls
        # ingressClassName: nginx

  ingress:
    dashboard:
      host:
        name: ceph.eyrie
      tls:
        - hosts:
          - ceph.eyrie
          secretName: ceph-dashboard-certs
      ingressClassName: nginx

  toolbox:
    enabled: true
    tolerations: *tolerations
    affinity:
      nodeAffinity: *affinity
