---
node_role: storage
ssd_volume_size: 64Gi
ssd_volume_storage_class: data-crypt-block
hdd_volume_size: 256Gi
hdd_volume_storage_class: nest-crypt-block
service_name: "%{nest::kubernetes::service}"

.placement: &placement
  nodeAffinity: &affinity
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
              - "%{lookup('node_role')}"
  tolerations: &tolerations
    - key: role
      operator: Exists
      effect: NoSchedule

resources:
  # Disable Nest backup
  backup: {}

  # Define certs for dashboard
  certs:
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: "%{nest::kubernetes::service}-dashboard-certs"
      namespace: "%{nest::kubernetes::namespace}"
    spec:
      secretName: "%{nest::kubernetes::service}-dashboard-certs"
      issuerRef:
        name: eyrie
        kind: ClusterIssuer
      dnsNames:
        - "%{lookup('service_name')}.eyrie"

values:
  clusterName: "%{lookup('service_name')}"

  # Expose resources for prometheus
  monitoring:
    enabled: true

  cephClusterSpec:
    cephVersion:
      # Workaround known issue with 18.2.4 on ARM64
      # See: https://github.com/rook/rook/issues/14502
      image: quay.io/ceph/ceph:v18.2.2

    # Run on specified node_role
    placement:
      all: *placement

    # Remove resource requirements on these small, dedicated nodes
    resources: null

    dashboard:
      prometheusEndpoint: http://ceph-monitoring-prometheus:9090
      ssl: false

    # Run mon and osd on local storage, declaratively
    # See: https://rook.io/docs/rook/latest/CRDs/Cluster/pvc-cluster/#local-storage-example
    # See: https://github.com/rook/rook/blob/master/deploy/examples/cluster-on-local-pvc.yaml
    mon:
      volumeClaimTemplate:
        spec:
          storageClassName: data-crypt
          resources:
            requests:
              storage: 10Gi
    storage:
      useAllNodes: false # use storageClassDeviceSets
      onlyApplyOSDPlacement: true # control placement per set
      storageClassDeviceSets:
        - name: set1 # operator expects this name e.g. set2, set3
          count: 3
          portable: false
          placement: &ssd_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                        - "%{lookup('node_role')}"
            tolerations: *tolerations
            # Avoid scheduling multiple OSD pods to the same node
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
          preparePlacement: *ssd_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: ssd
              spec:
                resources:
                  requests:
                    storage: "%{lookup('ssd_volume_size')}"
                storageClassName: "%{lookup('ssd_volume_storage_class')}"
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
        - name: set2
          count: 3
          portable: false
          placement: &hdd_osd_placement
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                        - workstation
            tolerations: *tolerations
            # Avoid scheduling multiple OSD pods to the same node
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                            - rook-ceph-osd-prepare
                    topologyKey: kubernetes.io/hostname
          preparePlacement: *hdd_osd_placement
          volumeClaimTemplates:
            - metadata:
                name: data
                annotations:
                  crushDeviceClass: hdd
              spec:
                resources:
                  requests:
                    storage: "%{lookup('hdd_volume_size')}"
                storageClassName: "%{lookup('hdd_volume_storage_class')}"
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
    # end: storage
  # end: cephClusterSpec

  ingress:
    dashboard:
      host:
        name: "%{lookup('service_name')}.eyrie"
      tls:
        - hosts:
          - "%{lookup('service_name')}.eyrie"
          secretName: "%{nest::kubernetes::service}-dashboard-certs"
      ingressClassName: nginx

  toolbox:
    enabled: true
    tolerations: *tolerations
    affinity:
      nodeAffinity: *affinity

patches:
  10-pools:
    # Block
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-blockpool"
        - op: replace
          path: '/spec/deviceClass'
          value: ssd
      target:
        kind: CephBlockPool
        name: ceph-blockpool
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-block"
        - op: replace
          path: '/parameters/pool'
          value: "%{lookup('service_name')}-blockpool"
      target:
        kind: StorageClass
        name: ceph-block

    # Filesystem
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/spec/dataPools/0/deviceClass'
          value: ssd
        - op: add
          path: '/spec/metadataServer/placement'
          value: *placement
        - op: remove
          path: '/spec/metadataServer/resources'
      target:
        kind: CephFilesystem
        name: ceph-filesystem
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem-csi"
        - op: replace
          path: '/spec/filesystemName'
          value: "%{lookup('service_name')}-filesystem"
      target:
        kind: CephFilesystemSubVolumeGroup
        name: ceph-filesystem-csi
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/parameters/fsName'
          value: "%{lookup('service_name')}-filesystem"
        - op: replace
          path: '/parameters/pool'
          value: "%{lookup('service_name')}-filesystem-data0"
      target:
        kind: StorageClass
        name: ceph-filesystem

    # Object
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-objectstore"
        - op: replace
          path: '/spec/dataPool/deviceClass'
          value: hdd
        - op: replace
          path: '/spec/dataPool/failureDomain'
          value: osd
        - op: add
          path: '/spec/gateway/placement'
          value: *placement
        - op: remove
          path: '/spec/gateway/resources'
      target:
        kind: CephObjectStore
        name: ceph-objectstore
    - patch:
        - op: replace
          path: '/metadata/name'
          value: "%{lookup('service_name')}-bucket"
        - op: replace
          path: '/parameters/objectStoreName'
          value: "%{lookup('service_name')}-objectstore"
      target:
        kind: StorageClass
        name: ceph-bucket
