---
.placement: &placement
  nodeAffinity: &affinity
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
              - storage
  tolerations: &tolerations
    - key: role
      operator: Exists
      effect: NoSchedule

resources:
  # Disable Nest backup
  backup: {}

  # Define certs for dashboard
  certs:
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: ceph-dashboard-certs
      namespace: "%{nest::kubernetes::namespace}"
    spec:
      secretName: ceph-dashboard-certs
      issuerRef:
        name: eyrie
        kind: ClusterIssuer
      dnsNames:
        - ceph.eyrie

values:
  # Expose resources for prometheus
  monitoring:
    enabled: true

  cephClusterSpec:
    cephVersion:
      # Workaround known issue with 18.2.4 on ARM64
      # See: https://github.com/rook/rook/issues/14502
      image: quay.io/ceph/ceph:v18.2.2

    # Run on storage nodes
    placement:
      all: *placement

    dashboard:
      prometheusEndpoint: http://rook-ceph-monitoring-kube-prometheus:9090
      ssl: false

    # Run mon and osd on local storage, declaratively
    # See: https://rook.io/docs/rook/latest/CRDs/Cluster/pvc-cluster/#local-storage-example
    # See: https://github.com/rook/rook/blob/master/deploy/examples/cluster-on-local-pvc.yaml
    mon:
      count: 3
      allowMultiplePerNode: false
      volumeClaimTemplate:
        spec:
          storageClassName: data-crypt
          resources:
            requests:
              storage: 10Gi
    storage:
      useAllNodes: false # use storageClassDeviceSets
      storageClassDeviceSets:
        - name: set1 # operator expects this name e.g. set2, set3
          count: 4   # number of storage nodes
          portable: false
          # Do not schedule multiple OSD prep pods to the same node.  After the
          # PVs are provisioned, the storage driver will control the scheduling.
          # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#more-practical-use-cases
          preparePlacement:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
          volumeClaimTemplates:
            - spec:
                resources:
                  requests:
                    storage: 64Gi
                storageClassName: data-crypt-block
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
    # end: storage
  # end: cephClusterSpec

  ingress:
    dashboard:
      host:
        name: ceph.eyrie
      tls:
        - hosts:
          - ceph.eyrie
          secretName: ceph-dashboard-certs
      ingressClassName: nginx

  toolbox:
    enabled: true
    tolerations: *tolerations
    affinity: *affinity

patches:
  # Force these services onto storage nodes
  # (patch because it's not easy to access with values)
  10-placement:
    - patch:
      - op: add
        path: '/spec/metadataServer/placement'
        value: *placement
      target:
        group: ceph.rook.io
        version: v1
        kind: CephFilesystem
    - patch:
      - op: add
        path: '/spec/gateway/placement'
        value: *placement
      target:
        group: ceph.rook.io
        version: v1
        kind: CephObjectStore

  # Remove resource requirements on these small, dedicated nodes
  10-resources:
    - patch:
        - op: remove
          path: '/spec/resources'
      target:
        group: ceph.rook.io
        version: v1
        kind: CephCluster
    - patch:
        - op: remove
          path: '/spec/metadataServer/resources'
      target:
        group: ceph.rook.io
        version: v1
        kind: CephFilesystem
    - patch:
        - op: remove
          path: '/spec/gateway/resources'
      target:
        group: ceph.rook.io
        version: v1
        kind: CephObjectStore
